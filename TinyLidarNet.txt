                                             TinyLidarNet: 2D LiDAR-based End-to-End Deep Learning Model for
                                                               F1TENTH Autonomous Racing
                                               Mohammed Misbah Zarrar, Qitao Weng, Bakhbyergyen Yerjan, Ahmet Soyyigit, and Heechul Yun

                                                                                University of Kansas, Lawrence, KS
                                                                 {zarrar 1607, wengqt, yerjanb, ahmet.soyyigit, heechul.yun}@ku.edu


                                           Abstract— Prior research has demonstrated the effectiveness     models that take a 2D LiDAR scan (e.g., a 1081 dimensional
                                        of end-to-end deep learning for robotic navigation, where the      vector over a 270-degree field of view) as input and predict
arXiv:2410.07447v1 [cs.RO] 9 Oct 2024




                                        control signals are directly derived from raw sensory data. How-   control commands (e.g., throttle and steering) as output [12].
                                        ever, the majority of existing end-to-end navigation solutions
                                        are predominantly camera-based. In this paper, we introduce        However, these MLP-based models do not perform well
                                        TinyLidarNet, a lightweight 2D LiDAR-based end-to-end deep         at high speeds and do not generalize well across different
                                        learning model for autonomous racing. An F1TENTH vehicle           environments [12].
                                        using TinyLidarNet won 3rd place in the 12th F1TENTH Au-              In this work, we aim to address the following research
                                        tonomous Grand Prix competition, demonstrating its compet-         questions: (1) Can we develop a 2D LiDAR-based end-to-end
                                        itive performance. We systematically analyze its performance
                                        on untrained tracks and computing requirements for real-time       deep learning model that demonstrates competitive perfor-
                                        processing. We find that TinyLidarNet’s 1D Convolutional Neu-      mance in actual F1TENTH autonomous racing competitions?
                                        ral Network (CNN) based architecture significantly outperforms     (2) What level of computational power is needed to execute
                                        widely used Multi-Layer Perceptron (MLP) based architecture.       such a deep learning model and achieve competitive per-
                                        In addition, we show that it can be processed in real-time on      formance in racing? (3) Can we achieve good performance
                                        low-end micro-controller units (MCUs).
                                                                                                           on unseen tracks, both in the real world and in simulation,
                                                             I. I NTRODUCTION                              without additional data collection and retraining? In other
                                                                                                           words, how well does our end-to-end model generalize,
                                           In F1TENTH autonomous racing [1], [2], developing an            especially compared to prior MLP-based models?
                                        intelligent, yet computationally efficient control algorithm is       To this end, first, we present TinyLidarNet, an end-to-
                                        necessary and challenging due to constraints in size, weight,      end deep convolutional neural network (CNN) that directly
                                        and power. These systems must swiftly process sensor input         takes a raw 2D LiDAR scan as input and predicts throttle
                                        data to make real-time decisions and allow fast collision-         and steering control commands. TinyLidarNet is inspired
                                        free navigation of the ego-vehicle in various racing tracks.       by the PilotNet architecture [4], a vision-based end-to-end
                                        Traditional approaches, which involve a complex pipeline of        CNN model used in NVIDIA’s Dave 2 project to drive a
                                        perception, planning, and control algorithms, are challenging      real car in various real road conditions. Instead of using
                                        to apply in fast-paced autonomous racing due to high com-          2D convolutions, as in the original PilotNet, TinyLidarNet
                                        putational costs and sensitivity to perception and modeling        uses 1D convolutions to capture the spatial features of the
                                        errors, which can propagate and accumulate through the             incoming 2D LiDAR scans. TinyLidarNet shows competitive
                                        pipeline End-to-end (E2E) deep learning approaches [3]–            performance in an actual F1TENTH competition 1 with a
                                        [6] present a promising alternative as they can simplify           relatively small amount of training data collected on the
                                        the control pipelines, improve computational efficiency, and       competition track (Figure 1), following the standard behavior
                                        achieve high performance [6].                                      cloning approach [3]. Second, we find that TinyLidarNet
                                           In end-to-end approaches, a neural network replaces all or      is computationally efficient and can perform an inference
                                        a subset of perception, planning, and control algorithms in        in less than 1 millisecond on the NVIDIA Jetson NX
                                        the traditional control pipeline [3], [7]. Many prior work has     platform. We further find that it is even possible to execute
                                        explored end-to-end approaches for autonomous driving [4],         the network on a tiny MCU in real-time. Specifically, we
                                        [8], [9]. However, the majority of these prior works are           port the TinyLidarNet to an ESP32-S3 and Raspberry Pi
                                        vision-based approaches, which require a large amount of           Pico MCU and can achieve sub-50ms latencies (>20Hz)
                                        training data to achieve good performance and consistency          after applying a standard 8-bit integer quantization using
                                        and are susceptible to environmental factors such as lighting      TensorFlowLite-Micro [13]. Lastly, we show that the trained
                                        conditions [4], [8]. Several studies have investigated end-to-     model generalizes well on unseen tracks, both in the real
                                        end approaches based on 2D LiDAR (Light Detection and              world as well as in simulation, even without any additional
                                        Ranging) in the context of F1TENTH racing specifically [6],        rounds of data collection and augmentation regimens such as
                                        [10], [11]. Compared to cameras, 2D LiDARs generate fewer          DAgger [14]. This is in stark contrast with prior MLP-based
                                        data, making the training of 2D LiDAR models easier.               2D LiDAR models [6], [10], [11], which struggle to work
                                           Most prior 2D LiDAR-based end-to-end approaches for
                                        F1TENTH racing are based on multi-layer perceptron (MLP)             1 3rd place winner at 12th F1TENTH autonomous grand-prix competition
well at high speed and unseen tracks [12]. This is because       B. End-to-end Deep Learning for Autonomous Driving
TinyLidarNet’s 1D convolutional filters can better capture          Rather than relying on conventional modular systems, end-
the spatial features of the 2D LiDAR scans.                      to-end approaches utilize deep neural networks, transforming
   In summary, we make the following contributions:              sensor input data into control outputs [7]. The concept was
   • We present TinyLidarNet, a 2D LiDAR-based end-              initially introduced in the 1980s [15] with a compact 3-
      to-end CNN architecture for F1TENTH autonomous             layer fully connected neural network tailored for autonomous
      racing, which shows competitive performance in real        cars. Subsequent to that, the DARPA Autonomous Vehicle
      racing and generalizes well on unseen tracks. We release   (DAVE) project emerged in the early 2000s [16] and nu-
      the code and training data as open source 2 .              merous other projects followed the suit [4], [17]–[19]. An
   • We systematically explore the latency, accuracy, and        example is NVIDIA’s DAVE-2 project, featuring a 9-layer
      performance trade-offs of TinyLidarNet on contem-          CNN for end-to-end control, demonstrating its ability to drive
      porary embedded computing platforms. We show the           a real car in various road conditions [4]. Most prior works are
      feasibility of using a tiny MCU to run TinyLidarNet in     vision-based, taking raw image pixels as input and directly
      real-time.                                                 generating control commands as output. In [5], on the other
   • We provide extensive evaluation results of TinyLidar-       hand, a 3D LiDAR is used as the primary input sensor, which
      Net’s performance and generalizability, compared to        produces a 3D point cloud as input to the end-to-end model,
      the state-of-the-art 2D LiDAR-based end-to-end MLP         in conjunction with GPS and coarse-grain map information.
      models. We show that TinyLidarNet’s CNN architec-          Their end-to-end model directly processes the 3D point cloud
      ture is superior to conventional MLP architectures for     via 3D sparse convolutions and was shown to be able to
      processing 2D planner LiDAR input.                         produce robust steering controls in a real autonomous car.
                                                                    Several prior works have investigated 2D planner LiDAR-
         II. BACKGROUND AND R ELATED W ORK
                                                                 based end-to-end approaches in the context of F1TENTH
A. F1TENTH Autonomous Racing Competition                         racing [6], [10]–[12]. Compared to cameras or 3D LiDARs,
    The F1TENTH Autonomous Racing competition [1] is             the 2D planner LiDAR in F1TENTH racing cars (see Sec-
a competitive autonomous racing event to develop and test        tion III) generates significantly fewer amount data, mainly
algorithms for autonomous racing with 1/10th scale race cars.    the distances from the ego vehicle to the surrounding objects
It challenges the scaled race cars to autonomously navigate      or walls on the 2D plane, which makes it easier to process.
and complete races in a given race track as fast as possible     These prior works targeting F1TENTH racing are based on
without collision. These race cars come equipped with an         MLP models to process 2D LiDAR scans. However, in the
array of sensors, but the 2D planner LiDAR is the most           F1TENTH community, end-to-end approaches have not been
commonly used primary sensor for perception. The compe-          popular in actual competitions due to poor performance. For
tition provides a realistic and dynamic testing ground for       example, in the 12th F1TENTH Grand Prix, we were the
researchers and students in the field of autonomous vehicles,    only team that adopted an end-to-end approach. In [20], it is
robotics, and artificial intelligence. Figure 1 shows the race   shown that a CNN model can be trained more efficiently than
track used in the 12th F1TENTH Grand Prix competition            an MLP model in a deep reinforcement learning framework,
held in IEEE/ACM CPS-IoT Week 2023 3 .                           but it did not show high-speed racing capability in the real
                                                                 world. In a recent survey by the creators of the F1TENTH
                                                                 racing [7], it is noted that end-to-end approaches exhibit
                                                                 low achievable speed, non-ideal driving characteristics (e.g.,
                                                                 wobbly behavior), and low generalizability on unseen tracks,
                                                                 among other limitations. In this work, we challenge these
                                                                 preconceived weaknesses of end-to-end approaches in the
                                                                 context of F1TENTH Racing.
                                                                        III. F1TENTH P LATFORM AND 2D L I DAR
                                                                    Figure 2 shows the F1TENTH car utilized in all of our
                                                                 real-world experiments. The chassis of the car is based
                                                                 on a Traxxas Rally 1/10-scale radio-controlled car with an
                                                                 Ackermann steering mechanism. Its primary sensor is a
                                                                 Hokuyo UST-10LX 2D Planner LiDAR, which has a 270◦
                                                                 field of view with a range of up to 10 meters. This LiDAR
                                                                 device provides an angular resolution of 0.25◦ and operates
                                                                 at a scan frequency of 40Hz, generating 1081 data points
  Fig. 1: 12th F1TENTH Grand Prix Competition Track              represented as a 1D distance array, which serves as the input
                                                                 for our end-to-end model. Since the scan frequency is 40 Hz,
                                                                 naturally the deadline for prediction of speed and throttle
  2 https://github.com/CSL-KU/TinyLidarNet                       becomes 0.025 seconds or 25 milliseconds. For control, an
  3 https://cps2023-race.f1tenth.org                             open-source electronic speed control (ESC) board controls
                                                                              Steering
                                                                                                  Speed   Output Layer:
                                                                               Angle
                                                                                                          Steering Angle and
                                                                                                          Speed


                                                                         Neurons: 10
                                                                                                          fc4: fully-connected layer

                                                                         Neurons: 50
                                                                                                          fc3: fully-connected layer
                                                                         Neurons: 100
                                                                                                          fc2: fully-connected layer
                                                                         Neurons: 1792
                                                                                                          fc1: fully-connected layer


                                                                                                          Conv5: 64@28
                                                                                                          convolution layer

                                                                        Kernel: 3x1, Strides: 1
Fig. 2: F1TENTH platform with a Hokuyo UST-10LX 2D                                                        Conv4: 64@30
LiDAR and a NVIDIA Jetson Xavier NX on-board computer.                                                    convolution layer


                                                                        Kernel: 3x1, Strides: 1
                                                                                                          Conv3: 48@32
the RPM of a brushless motor and a steering servo. A power                                                convolution layer

board distributes power from a lithium polymer (LiPO)
                                                                        Kernel: 4x1, Strides: 2
battery to the sensors, motors, and the on-board computer.
For on-board computing, an NVIDIA Jetson Xavier NX                                                        Conv2: 36@66
                                                                                                          convolution layer
embedded computer is used to run all software, which equips
                                                                        Kernel: 8x1, Strides: 4
8 64-bit ARM CPU cores and a 384-core Volta GPU. On the
                                                                                                          Conv1: 24@268
software side, we use the ROS Noetic Ninjemys framework                                                   convolution layer
on Ubuntu 20.04.6 operating system.
                                                                        Kernel:10x1, Strides: 4           Input: 1081 x 1
                                                                                                          Lidar Scan
                   IV. T INY L IDAR N ET
  In this section, we introduce the TinyLidarNet architecture   Fig. 3: TinyLidarNet architecture: 9 layers (5 convolutional,
and our data collection and training process.                   4 fully-connected) with 220,686 parameters.

A. Architecture
   Figure 3 shows the architecture of TinyLidarNet. The         85% (10,478 samples) of the collected samples to training
architecture is comprised of 9 layers (5 convolutional and 4    and the remaining portion to testing and validation. We used
fully connected) with a total of 220K parameters and takes      a batch size of 64 and trained the network for 20 epochs
about 1.5 million multiply-accumulate operations (MACs)         with a learning rate of 5e-5 and Huber loss [21].
to execute. The architecture is inspired by the PilotNet           The TinyLidarNet training process is performed locally on
architecture [4], which is a vision-based end-to-end model      the NVIDIA Jetson Xavier NX platform and takes approxi-
for NVIDIA’s Dave2 self-driving car. The main differences       mately 4 minutes to complete.
are that it takes a 2D LiDAR scan, instead of an RGB               During the data collection process, we found that the
image, as input and uses 1D CNN layers instead of 2D ones.      quality of the LiDAR scan samples deteriorated significantly
Quantitatively, compared to PilotNet [4], TinyLidarNet has a    when the car navigated dark track sections. LiDAR sensors
similar number of parameters (250K vs. 220K) but requires       tend to perform less effectively on black surfaces, resulting in
significantly lower MACs (27 million vs. 1.5 million) be-       noisy point clouds [22]. To reduce noise, we employ median
cause the input dimensionality is much lower (200x66 RGB        and interpolation filters. These filters play a crucial role in
pixels vs. 1081 LiDAR scan samples). As such, it has a          stabilizing point clouds.
significantly lower computational demand, which makes it
                                                                                            V. E VALUATION
possible to use less powerful computing platforms, as we
investigate in Section V-C.                                       In this section, we evaluate the performance of TinyLidar-
                                                                Net.
B. Data Collection, Pre-processing and Training
   The data collection is carried out by driving the vehicle    A. Insights from an F1TENTH Competition
manually using a joystick. The collected data includes servo       Using TinyLidarNet, we participated in the 12th
angle and speed data from the ESC board of the car and the      F1TENTH Autonomous Grand Prix competition and won
1D range array (1081 values) from the Hokuyo 2D LiDAR.          the 3rd place award out of 13 participating teams. Although
   We collected a total of 12,329 samples, approximately 5      TinyLidarNet did not secure the top position, it exhibited
minutes of driving on the training track, shown in Figure 1.    competitive performance and demonstrated several advan-
Note that during the data collection, the car drove with        tages over traditional approaches that relied on perception,
stationary obstacles (opponent vehicles) present. We assigned   planning, and control pipelines.
   First, during the competition, the track depicted in Figure 1         Model           Input dim.   Parameters     MACs
underwent frequent alterations due to collisions. Each time              TinyLidarNetL      1081        220,686     1,546,960
a racecar collided with the track, the track was manually                TinyLidarNetM       541        111,886      687,680
adjusted, leading to slight changes in its configuration. This           TinyLidarNetS       271         54,286      240,752
                                                                         MLP256L [12]       1081        343,298      342,784
posed a considerable challenge for participants who relied on            MLP256M             541        205,058      204,544
an offline map for path planning. In contrast, TinyLidarNet              MLP256S             271        135,938      135,424
does not rely on an offline map and was largely immune to
the alterations in the track configuration.                               TABLE I: Comparison of end-to-end models.
   Second, a particularly interesting capability of TinyLi-
darNet is its ability to overtake other cars during head-
to-head races in the tournament. This is interesting be-           LiDAR scan as input. TinyLidarNetM and TinyLidarNetS are
cause the training dataset comprises examples of the car           smaller variants, which down-sample the LiDAR scan by tak-
driving on the competition track by a human driver with            ing one for every 2 or 4 values of the range data, respectively.
static obstacles only, as detailed in Section IV-B. Note that      Except for the input dimension, they have identical 9-layer
executing overtaking maneuvers in high-speed racing is a           architecture. On the other hand, MLP256L [12] is a multi-
challenging task for classical control approaches [23], [24],      layer perceptron with 2 hidden layers, with 256 neurons in
and most participants in the competition did not attempt to        each layer. MLP256M and MLP256S are its smaller variants
execute overtaking maneuvers during the race. A team that          with smaller input dimensions.
implemented an overtaking algorithm did so only in the long           Figure 4 shows the four tracks we use for evaluation.
straight section of the track (Figure 1). On the other hand,       These tracks are widely used in F1TENTH literature [25],
TinyLidarNet was able to overtake moving opponents in any          [26], which we use without any modifications.
part of the track throughout the competition, likely because          The basic evaluation setup is as follows. First, we train
moving race cars were recognized as static obstacles or walls.     all compared models using the same dataset, which was
   Overall, the competitive performance of TinyLidarNet in         originally collected on the F1TENTH competition, as de-
the competition motivated us to systematically analyze its         scribed in Section IV-B, and evaluate them on the four
performance and characteristics.                                   tracks using a simulator infrastructure in [27]. Using the
                                                                   simulation infrastructure, each end-to-end model drives a
B. Performance on Simulated Tracks
                                                                   simulated car for one complete lap on a track for 10 trials,
  In this subsection, we evaluate TinyLidarNet’s perfor-           each time starting from a random position on the track. For
mance on unseen virtual tracks.                                    each successful completion among the 10 trials, the average
                                                                   lap time was calculated. In addition, the average progress
                                                                   rate measures the percentage of the track the model can
                                                                   progress, on average, during the 10 trials.
                                                                      To mimic real-world conditions, we adjusted the simula-
                                                                   tion so that the control frequency is set to 40 Hz. We also
                                                                   add random Gaussian noise in the range of 0 to 0.5 to the
                                                                   LiDAR scans and perform a range clipping at 10 m to mimic
                                                                   the behavior of Hokuyo UST-10LX LiDAR in the F1TENTH
                                                                   platform, which has a range of 10 m.
(a) F1TENTH Gym (GYM)                    (b) Austin (AUS)
                                                                      Note that a common practice of training an end-to-end
                                                                   model for a robot involves starting in simulation and even-
                                                                   tually deploying it in the real world. However, bridging
                                                                   the Simulation to Reality gap (Sim2Real gap) can pose
                                                                   significant challenges [28], [29]. On the other hand, our
                                                                   evaluation strategy can be described as Real2Sim as we train
                                                                   the models using the real-world track dataset and test them
                                                                   in a simulated environment.
                                                                      Table II shows the evaluation results. Note first that all
                                                                   three TinyLidarNetL,M,S can complete all 10 laps without
    (c) Moscow (MOS)              (d) Spielberg (SPL)              any collision, hence achieving 100% average progress, in all
                                                                   four tested tracks. On the other hand, MLP256 models strug-
Fig. 4: Simulation Tracks from F1TENTH gym and                     gle to complete the laps. MLP256 models could complete the
F1TENTH racetrack repository [25], [26]                            lab some of the times, but their success rates are not close
                                                                   to 100% in most cases. The average lap times of MLP256
   Table I shows the basic characteristics of the compared 2D      models are also somewhat slower than that of TinyLidarNet
Lidar-based end-to-end models. Note that TinyLidarNetL is          models in many cases. Overall, the TinyLidarNet variants
the original version that we use in an actual competition V-A.     show better performance, especially in terms of average
It is also the largest one as it takes all 1081 range data of a    progress, than the MLP256 variants.
                                                                     Average Lap Time (s)               Average Progress (%)
                                           Model                   GYM AUS      MOS      SPL          GYM   AUS     MOS     SPL
                                           TinyLidarNetL           25.8       85.7    63.3    65.3    100     100       100    100
                                           TinyLidarNetM           25.3       80.0    59.5    61.5    100     100       100    100
                                           TinyLidarNetS           26.9       83.4    61.8    64.1    100     100       100    100
                                           MLP256L [12]            N/A        N/A     58.8    58.3     31      16        42     61
                                           MLP256M                 28.4       N/A     64.3    65.7    100      17        58    78
                                           MLP256S                 27.6       N/A     N/A     62.2     77      48        29     37

 TABLE II: Average lap time (of successful trials) and the average progress (of all trials). At each trial, the ego-vehicle’s
 position in the track is randomly chosen.

                                 MLP256m           TinyLidarNetm



              8
Speed [m/s]




              6


                                                                                                                        MLP256m
              4
                                                                                                                                                   8

                                                                                                                                                   7
              2

                                                                                                                                                   6
                  0       20          40               60             80             100




                                                                                                                                                       Speed [m/s]
                                      Track progress [%]
                                                                                                                                                   5
 Fig. 5: Speed comparison of different models on GYM Track                                                                                         4

                                                                                                                                                   3

    The Figure 5 and Figure 6 show the driving speed                                                                                               2
 and the trajectory, respectively, of the TinyLidarNetM and                                                                                        1
 MLP256M models in one of the successfully completed laps                                                           TinyLidarNetm
 out of 10 trials on the GYM track. Note that MLP256M ’s
 speed fluctuates significantly and its trajectory is wobbly. In
 contrast, TinyLidarNetM maintains a more consistent speed
 and stable trajectory. Note that in terms of average progress,
 all TinyLidarNet models consistently outperform MLP256
 models in all evaluated tracks. Regarding average lap time,                                 Fig. 6: A snippet of the trajectory of different models with
 TinyLidarNet models are usually faster than or similar to                                   speed profile on GYM track.
 MLP256 models.
    In summary, we show that TinyLidarNet models, trained
 on real-world data, are well generalized to navigate unseen                                 computer platform of our F1TENTH platform, which fea-
 race tracks, while the commonly used MLP models struggle                                    tures powerful six ARM CPU cores running at 1.9 GHz. We
 to generalize. This is because the CNN layers of TinyLidar-                                 also use XIAO ESP32-S3 [30] and Raspberry Pi Pico [31]
 Net can capture spatial features of LiDAR scans, enabling                                   MCUs to understand the potential of using these inexpensive
 better generalization.                                                                      low-end computing platforms for the real-time processing
 C. Inference Latency                                                                        of TinyLidarNet. To this end, we port all three versions
    In the context of racing, latency is of paramount impor-                                 of TinyLidarNet on the MCUs and measure the inference
 tance due to the inherent stringent real-time requirements in                               latency of the models.
 such applications. In this subsection, we evaluate the infer-
                                                                                                 Model                    Xavier NX   ESP32-S3   RPi Pico
 ence latency of TinyLidarNet models on different computing
 platforms.                                                                                      TinyLidarNetL (fp32)         <1        838       2642
                                                                                                 TinyLidarNetL (int8)         <1         16        196
                                                                                                 TinyLidarNetM (int8)         <1         8         91
                          Xavier NX           ESP32-S3                    RPi Pico
                                                                                                 TinyLidarNetS (int8)         <1         4         36
                        NVIDIA Carmel        Xtensa LX7            ARM Cortex-M0+
              CPU
                         6C@1.9 GHz         2C@240 MHz              2C@133 Mhz                       TABLE IV: Inference latency (ms) comparison
              Memory    8GB LPDDR4x         8MB PSRAM               264KB SRAM
              Storage    16GB eMMC           8MB Flash                2MB Flash
                                                                                                Table IV shows the results. Note that (fp32) refers to the
                         TABLE III: Computing platforms                                      baseline 32-bit floating point number-based models whereas
                                                                                             (int8) refers to 8-bit integer quantized models to reduce
   Table III shows the three computing platforms we use for                                  computational overhead and storage space demand to store
 evaluation. Note that Jetson Xavier NX is the main onboard                                  the weights of the model.
   First, note that all TinyLidarNet models are small enough                                     Average       Success
                                                                              Model
                                                                                               Lap Time (s)   Rate (%)
to fit in both MCUs. Even in the smaller Raspberry Pi
Pico MCU with only 264KB SRAM and 2MB Flash, all                              TinyLidarNetL        20.5          100
                                                                              TinyLidarNetM        19.9          100
TinyLidarNet models can fit without any issue.
                                                                              TinyLidarNetS        19.5           80
   Second, the inference latency of TinyLidarNetL (fp32),                     MLP256L [12]         N/A            0
however, is more than 2 seconds on the Pico and more than                     MLP256M              N/A             0
800 ms on the ESP32-S3 MCU, which is more powerful than                       MLP256S              N/A             0
Pico but still significantly limited compared to Xavier NX.
                                                                  TABLE V: Average lap time and success rate of the models
   Third, using 8-bit quantized models enables TinyLidarNet
                                                                  on the real track. The average lap time was calculated from
to be executed in real-time as the TinyLidarNetL (int8) model
                                                                  the successful lap completion of 5 trials by placing the ego
takes only 16 ms to perform an inference on the ESP32-S3.
                                                                  vehicle in random positions on the map.
This means that the model can run at >50Hz. The inference
latency of the smallest TinyLidarNetS (int8) model is only
36 ms (>20Hz) on the Pico, which is sufficient for real-time
control for most robotics applications.                           U-shaped turns at the bottom left and top right corners of
   In summary, with a standard quantization-based optimiza-       the track shown in Figure 7.
tion, we show that TinyLidarNet is lightweight enough to             In contrast, all three versions of TinyLidarNet were able to
run on low-end MCUs in real-time.                                 complete the lap. TinyLidarNetS crashed only once (out of
                                                                  five trials), possibly because it is the smallest of all and loses
D. Performance on Unseen Real Tracks                              3/4 of the LiDAR range information due to downsampling
                                                                  (skipping three out of every four range values). Nevertheless,
   In this subsection, we evaluate the performance of TinyL-      all TinyLidarNet models clearly outperform the MLP model.
idarNet on an unseen real track to validate the findings on          As discussed earlier, TinyLidarNet’s superior performance
the simulated tracks in Section V-B.                              can be attributed from its use of 1D CNN layers, which
   As in Section V-B, the evaluated models are all trained        can capture spatial features of 2D LiDAR scans better. As a
using the dataset we collected during the F1TENTH com-            result, TinyLidarNet models generalize well on unseen tracks
petition, described in Section IV-B. The trained models are       as well as unseen moving vehicles, all of which have distinct
then tested on a different track installed in our lab. Figure 1   spatial features that can be learned. While it is well-known
shows the track.                                                  that CNN is good at processing vision data, our results
                                                                  strongly suggest that it is also good at processing 2D LiDAR
                                                                  data, which is consistent with recent findings in [20].

                                                                                        VI. C ONCLUSION

                                                                     In this study, we introduced TinyLidarNet, a lightweight
                                                                  2D LiDAR-based end-to-end deep learning model designed
                                                                  for F1TENTH racing. TinyLidarNet takes an array of LiDAR
                                                                  scans as direct input and predicts steering and speed through
                                                                  a 1D convolutional neural network.
                                                                     We conducted a comprehensive evaluation on the gener-
                                                                  alizability of the model on unseen tracks both in simulation
                                                                  and in the real-world. We showed that TinyLidarNet’s 1D
                Fig. 7: Real-world test track                     CNN-based architecture is superior to the commonly used
                                                                  MLP architecture in processing 2D LiDAR scan data.
   We employed a similar performance metric to what was              We also evaluated the effects of quantization on TinyLi-
discussed in Section V-B, conducting these experiments over       darNet. Deploying it on two low-cost MCUs, we found its
5 trials with the ego vehicle placed on the track at 5 randomly   inference to be sufficiently efficient for autonomous vehicle
selected starting positions, indicated by the red markers         control, demonstrating its lightweight nature and low latency.
visible in Figure 7. To ensure a fair comparison, we set the         Future research avenues include exploring the use of
minimum speed to -0.5 m/s, considering the vehicle’s inertia,     DAgger [14], [32] to improve data collection, and using
and capped the maximum speed limit at 5 m/s, aligning with        TinyLidarNet as a foundation for bootstrapping with Deep
the linearly mapped data range from 0 m/s to 5 m/s used           Reinforcement Learning techniques, further enhancing its
during training for all models.                                   capabilities and adaptability in challenging racing scenarios.
   Table V shows the results. Across five trials, all MLP256
variants consistently crashes, highlighting the struggle to                           ACKNOWLEDGMENTS
generalize on a new track that was not seen during training.
All three MLP256 models were unable to complete a single            This research is supported in part by the NSF grant CPS-
lap and struggled especially when they needed to take hard        2038923.
                              R EFERENCES                                        [23] S. Bak, J. Betz, A. Chawla, H. Zheng, and R. Mangharam, “Stress
                                                                                      testing autonomous racing overtake maneuvers with rrt,” in 2022 IEEE
 [1] F1Tenth, “F1/10 autonomous racing competition.” http:                            Intelligent Vehicles Symposium (IV), pp. 806–812, IEEE, 2022.
     //f1tenth.org.                                                              [24] J. Zhang and H.-W. Loidl, “F1tenth: An over-taking algorithm using
 [2] M. O’Kelly, H. Zheng, D. Karthik, and R. Mangharam, “F1tenth:                    machine learning,” in International Conference on Automation and
     An open-source evaluation environment for continuous control and                 Computing (ICAC), pp. 01–06, IEEE, 2023.
     reinforcement learning,” Proceedings of Machine Learning Research,          [25] M. O’Kelly, H. Zheng, D. Karthik, and R. Mangharam, “F1tenth: An
     vol. 123, 2020.                                                                  open-source evaluation environment for continuous control and rein-
 [3] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training              forcement learning,” in NeurIPS 2019 Competition and Demonstration
     of deep visuomotor policies,” Journal of Machine Learning Research,              Track, pp. 77–89, PMLR, 2020.
     2016.                                                                       [26] J. Betz, H. Zheng, A. Liniger, U. Rosolia, P. Karle, M. Behl, V. Krovi,
 [4] M. Bojarski et al., “End-to-End Learning for Self-Driving Cars,”                 and R. Mangharam, “Autonomous vehicles on the edge: A survey on
     arXiV, 2016.                                                                     autonomous vehicle racing,” IEEE Open J. Intell. Transp. Syst., vol. 3,
 [5] Z. Liu, A. Amini, S. Zhu, S. Karaman, S. Han, and D. L. Rus,                     pp. 458–488, 2022.
     “Efficient and robust lidar-based end-to-end navigation,” in 2021           [27] B. D. Evans, R. Trumpp, M. Caccamo, H. W. Jordaan, and H. A.
     IEEE International Conference on Robotics and Automation (ICRA),                 Engelbrecht, “Unifying f1tenth autonomous racing: Survey, methods
     pp. 13247–13254, IEEE, 2021.                                                     and benchmarks,” 2024.
                                                                                 [28] P. Trentsios, M. Wolf, and D. Gerhard, “Overcoming the sim-to-real
 [6] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, “End-to-end
                                                                                      gap in autonomous robots,” Procedia CIRP, vol. 109, pp. 287–292,
     autonomous driving: Challenges and frontiers,” 2023.
                                                                                      2022. 32nd CIRP Design Conference (CIRP Design 2022) - Design
 [7] J. Betz, H. Zheng, A. Liniger, U. Rosolia, P. Karle, M. Behl, V. Krovi,
                                                                                      in a changing world.
     and R. Mangharam, “Autonomous vehicles on the edge: A survey
                                                                                 [29] X. Hu, S. Li, T. Huang, B. Tang, R. Huai, and L. Chen, “How
     on autonomous vehicle racing,” IEEE Open Journal of Intelligent
                                                                                      simulation helps autonomous driving: A survey of sim2real, digital
     Transportation Systems, vol. 3, pp. 458–488, 2022.
                                                                                      twins, and parallel intelligence,” IEEE Transactions on Intelligent
 [8] M. Bojarski, C. Chen, J. Daw, A. Değirmenci, J. Deri, B. Firner,                Vehicles, vol. 9, no. 1, pp. 593–612, 2024.
     B. Flepp, S. Gogri, J. Hong, L. Jackel, Z. Jia, B. Lee, B. Liu,             [30] “Seed Studio XIAO ESP32S3.” https://www.seeedstudio.
     F. Liu, U. Muller, S. Payne, N. K. N. Prasad, A. Provodin, J. Roach,             com/XIAO-ESP32S3-p-5627.html.
     T. Rvachov, N. Tadimeti, J. van Engelen, H. Wen, E. Yang, and               [31] “Raspberry      Pi    Pico    and    Pico     W.”    https://www.
     Z. Yang, “The nvidia pilotnet experiments,” 2020.                                raspberrypi.com/documentation/microcontrollers/
 [9] Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. Theodorou, and                raspberry-pi-pico.html.
     B. Boots, “Agile autonomous driving using end-to-end deep imitation         [32] M. Kelly, C. Sidrane, K. Driggs-Campbell, and M. J. Kochenderfer,
     learning,” 2019.                                                                 “Hg-dagger: Interactive imitation learning with human experts,” in
[10] F. Codevilla, M. Müller, A. López, V. Koltun, and A. Dosovitskiy,              2019 International Conference on Robotics and Automation (ICRA),
     “End-to-end driving via conditional imitation learning,” 2018.                   pp. 8077–8083, 2019.
[11] S. N. Wadekar, B. J. Schwartz, S. S. Kannan, M. Mar, R. K. Manna,
     V. Chellapandi, D. J. Gonzalez, and A. E. Gamal, “Towards end-to-
     end deep learning for autonomous racing: On data collection and a
     unified architecture for steering and throttle prediction,” 2021.
[12] X. Sun, M. Zhou, Z. Zhuang, S. Yang, J. Betz, and R. Mangharam,
     “A benchmark comparison of imitation learning-based control policies
     for autonomous racing,” in 2023 IEEE Intelligent Vehicles Symposium
     (IV), pp. 1–5, IEEE, 2023.
[13] R. David, J. Duke, P. Warden, et al., “Tensorflow lite micro:
     Embedded machine learning on tinyml systems,” arXiv preprint
     arXiv:2010.08678, 2020.
[14] S. Ross, G. Gordon, and D. Bagnell, “A reduction of imitation learning
     and structured prediction to no-regret online learning,” in Proceedings
     of the fourteenth international conference on artificial intelligence and
     statistics, pp. 627–635, JMLR Workshop and Conference Proceedings,
     2011.
[15] D. a. Pomerleau, “ALVINN: An autonomous land vehicle in a neural
     network,” in Advances in Neural Information Processing Systems
     (NIPS), 1989.
[16] Y. Lecun, E. Cosatto, J. Ben, U. Muller, and B. Flepp, “DAVE:
     Autonomous off-road vehicle control using end-to-end learning,” Tech.
     Rep. DARPA-IPTO Final Report, Courant Institute/CBLL, 2004.
[17] M. G. Bechtel, E. McEllhiney, M. Kim, and H. Yun, “Deeppicar:
     A low-cost deep neural network-based autonomous car,” in IEEE
     International Conference on Embedded and Real-Time Computing
     Systems and Applications (RTCSA), 2018.
[18] M. Bechtel, Q. Weng, and H. Yun, “Deeppicarmicro: Applying tinyml
     to autonomous cyber physical systems,” in 2022 IEEE 28th Interna-
     tional Conference on Embedded and Real-Time Computing Systems
     and Applications (RTCSA), pp. 120–127, IEEE, 2022.
[19] T. Weiss and M. Behl, “Deepracing: A framework for autonomous
     racing,” in 2020 Design, automation & test in Europe conference &
     exhibition (DATE), pp. 1163–1168, IEEE, 2020.
[20] M. Bosello, R. Tse, and G. Pau, “Train in austria, race in montecarlo:
     Generalized rl for cross-track f1 tenth lidar-based races,” in 2022 IEEE
     19th Annual Consumer Communications & Networking Conference
     (CCNC), pp. 290–298, IEEE, 2022.
[21] P. J. Huber, “Robust estimation of a location parameter,” Annals of
     Mathematical Statistics, vol. 35, pp. 492–518, 1964.
[22] S. Wu, G. K. Reddy, and D. Banerjee, “Pitch-black nanostructured
     copper oxide as an alternative to carbon black for autonomous envi-
     ronments,” Advanced Intelligent Systems, vol. 3, no. 9, p. 2100049,
     2021.
